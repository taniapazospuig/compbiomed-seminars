{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/full_data_corrected_2024.pkl', 'rb') as file:\n",
    "    full_data = pickle.load(file)\n",
    "\n",
    "print(type(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples (same for all keys)\n",
    "print(len(full_data['PVC_transition']))\n",
    "# Convert to a dataframe for easier manipulation\n",
    "\n",
    "metadata_keys = ['Sex', 'HTA', 'Age', 'PVC_transition', 'SOO_chamber', 'Height', 'Weight', 'BMI', \n",
    "                 'DM', 'DLP', 'Smoker', 'COPD', 'Sleep_apnea', 'CLINICAL_SCORE', 'SOO', 'OTorigin']\n",
    "\n",
    "df_meta = pd.DataFrame({key: full_data[key] for key in metadata_keys})\n",
    "df_meta = df_meta.drop(columns=['CLINICAL_SCORE'])\n",
    "\n",
    "\n",
    "df_meta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build multi-lead ECG array\n",
    "ecg_leads = ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "n_samples = len(full_data['I'])\n",
    "signal_length = len(full_data['I'][0])  # assuming all leads same length\n",
    "\n",
    "multi_lead_ecgs = np.zeros((n_samples, len(ecg_leads), signal_length))\n",
    "for i, lead in enumerate(ecg_leads):\n",
    "    for j in range(n_samples):\n",
    "        multi_lead_ecgs[j, i, :] = full_data[lead][j]\n",
    "\n",
    "# Check the shape of the multi-lead ECG array\n",
    "print(multi_lead_ecgs.shape)  # should be (n_samples, ecg_leads, signal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a full ECG record, including:\n",
    "* Raw 12-lead signals, each one a NumPy array\n",
    "* Patient metadata: Sex, HTA, PVC_transition, SOO_chamber, Height, Weight, BMI, DM, DLP, Smoker, COPD, Sleep_apnea, CLINICAL_SCORE, SOO, OTorigin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first ECG sample (12 leads)\n",
    "sample_idx = 0\n",
    "fig, axs = plt.subplots(6, 2, figsize=(12, 10))\n",
    "fig.suptitle(f'ECG Sample {sample_idx}', fontsize=16)\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.plot(multi_lead_ecgs[sample_idx, i])\n",
    "    ax.set_title(ecg_leads[i])\n",
    "    ax.set_xlim([0, signal_length])\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as sp\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Processes one sample ECG with all 12 leads\n",
    "def preprocess_ecg_signal(ecg_signals, fs=1000, target_fs=250, high=0.5, low=100.0):\n",
    "    \"\"\"\n",
    "    Preprocess a multi-lead ECG signal [timepoints, leads]:\n",
    "    - Resample to target_fs\n",
    "    - Bandpass filter between `high` and `low`\n",
    "    \"\"\"\n",
    "    timepoints = ecg_signals.shape[0] # Initially, 2500\n",
    "    new_timepoints = int(timepoints * target_fs / fs) # 625\n",
    "    \n",
    "    # Resample each lead using interpolation\n",
    "    ecg_resampled = np.zeros((new_timepoints, ecg_signals.shape[1]))\n",
    "    for lead in range(ecg_signals.shape[1]):\n",
    "        f = interp1d(np.arange(timepoints), ecg_signals[:, lead]) # Interpolation function from original points\n",
    "        ecg_resampled[:, lead] = f(np.linspace(0, timepoints - 1, new_timepoints)) # Create the new timeline\n",
    "\n",
    "    # Apply high-pass filter (remove slow drifts below 0.5 Hz)\n",
    "    b_high, a_high = sp.butter(2, high / (target_fs / 2), btype='high')\n",
    "    ecg_filtered = sp.filtfilt(b_high, a_high, ecg_resampled, axis=0)\n",
    "\n",
    "    # Apply low-pass filter (remove noise above 100 Hz)\n",
    "    b_low, a_low = sp.butter(2, low / (target_fs / 2), btype='low')\n",
    "    ecg_filtered = sp.filtfilt(b_low, a_low, ecg_filtered, axis=0)\n",
    "\n",
    "    return ecg_filtered # Return the signal with shape [625, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_r_peak(ecg_filtered, fs=250):\n",
    "    \"\"\"\n",
    "    Align ECG segment around R peak detected in the second second (samples 250–500).\n",
    "    Returns a centered window of 1.25s (312 samples) around the R peak.\n",
    "    \"\"\"\n",
    "    lead_for_r = 1  # Lead II is commonly used for R-peak detection\n",
    "    signal = ecg_filtered[:, lead_for_r]\n",
    "    \n",
    "    # Focus on 1s to 2s segment\n",
    "    window_start, window_end = int(1 * fs), int(2 * fs)\n",
    "    segment = signal[window_start:window_end]\n",
    "    \n",
    "    # Find R peak in that window\n",
    "    r_peak_relative = np.argmax(segment)\n",
    "    r_peak_absolute = window_start + r_peak_relative\n",
    "\n",
    "    # Center a 312-sample window on R peak\n",
    "    half_window = 156\n",
    "    start_idx = max(0, r_peak_absolute - half_window)\n",
    "    end_idx = start_idx + 312\n",
    "\n",
    "    # If window is beyond bounds, pad accordingly\n",
    "    if end_idx > ecg_filtered.shape[0]:\n",
    "        pad_len = end_idx - ecg_filtered.shape[0]\n",
    "        padded = np.pad(ecg_filtered, ((0, pad_len), (0, 0)), mode='constant')\n",
    "        segment_aligned = padded[start_idx:end_idx]\n",
    "    else:\n",
    "        segment_aligned = ecg_filtered[start_idx:end_idx]\n",
    "\n",
    "    return segment_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_ecgs = []\n",
    "for i in range(multi_lead_ecgs.shape[0]):\n",
    "    signal_raw = multi_lead_ecgs[i].T  # shape becomes [2500, 12]\n",
    "    processed = preprocess_ecg_signal(signal_raw)  # shape [625, 12]\n",
    "    aligned = align_to_r_peak(processed)  # shape [312, 12]\n",
    "    preprocessed_ecgs.append(aligned)\n",
    "\n",
    "preprocessed_ecgs = np.stack(preprocessed_ecgs)  # shape [n_samples, 312, 12]\n",
    "print(\"All signals aligned to R peak. Final shape:\", preprocessed_ecgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples to plot\n",
    "n_samples_to_plot = 6\n",
    "lead_index = 1  # Lead II\n",
    "lead_name = 'II'\n",
    "\n",
    "# Randomly select sample indices\n",
    "np.random.seed(42)  # For reproducibility\n",
    "sample_indices = np.random.choice(preprocessed_ecgs.shape[0], n_samples_to_plot, replace=False)\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(n_samples_to_plot, 1, figsize=(12, 2.5 * n_samples_to_plot), sharex=True)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    axs[i].plot(preprocessed_ecgs[idx, :, lead_index], label=f'Sample {idx}')\n",
    "    axs[i].axvline(x=156, color='r', linestyle='--', label='Center (R-peak)')\n",
    "    axs[i].set_ylabel(\"Amplitude\")\n",
    "    axs[i].grid(True)\n",
    "    axs[i].legend(loc='upper right')\n",
    "    axs[i].set_title(f\"Aligned ECG - Sample {idx} (Lead {lead_name})\")\n",
    "\n",
    "axs[-1].set_xlabel(\"Sample Index (Aligned Window)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the raw signal vs. aligned signal for a given sample\n",
    "sample_idx = i  # make sure 'i' is defined or choose another valid sample index\n",
    "\n",
    "leads_to_plot = ['I', 'AVR', 'V2']\n",
    "lead_indices = [ecg_leads.index(lead) for lead in leads_to_plot]\n",
    "\n",
    "# Raw signal: shape [12, 2500]\n",
    "raw_signal = multi_lead_ecgs[sample_idx]\n",
    "\n",
    "# Aligned signal (after preprocessing and R-peak alignment): shape [312, 12]\n",
    "aligned_signal = preprocessed_ecgs[sample_idx].T  # shape [12, 312]\n",
    "\n",
    "# Time axes\n",
    "t_raw = np.linspace(0, 2.5, raw_signal.shape[1])              # 2500 samples at 1000 Hz\n",
    "t_aligned = np.linspace(0, 1.25, aligned_signal.shape[1])     # 312 samples at 250 Hz\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for k, lead_idx in enumerate(lead_indices):\n",
    "    plt.subplot(len(lead_indices), 1, k+1)\n",
    "    \n",
    "    plt.plot(t_raw, raw_signal[lead_idx], label='Raw (1000Hz)', alpha=0.6)\n",
    "    plt.plot(t_aligned, aligned_signal[lead_idx], label='Aligned (250Hz)', alpha=0.9)\n",
    "    \n",
    "    plt.title(f\"Lead {ecg_leads[lead_idx]} - Sample {sample_idx}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map SOO to chamber (Left / Right /OTHER)\n",
    "# Load Hoja1 and Hoja2 from the mapping Excel file\n",
    "labels_path = \"data/labels_FontiersUnsupervised.xlsx\"\n",
    "map_hoja1 = pd.read_excel(labels_path, sheet_name=\"Hoja1\")\n",
    "map_hoja2 = pd.read_excel(labels_path, sheet_name=\"Hoja2\")\n",
    "\n",
    "# Build lookup dictionaries\n",
    "map_1 = dict(zip(map_hoja1[\"SOO\"], map_hoja1[\"SOO_Chamber\"]))\n",
    "map_2 = dict(zip(map_hoja2[\"SOO\"], map_hoja2[\"SOO_chamber\"]))\n",
    "\n",
    "# Step 1: Initial mapping using Hoja1\n",
    "simplified_chambers = []\n",
    "for entry in full_data[\"SOO\"]:\n",
    "    if isinstance(entry, str) and entry in map_1:\n",
    "        simplified_chambers.append(map_1[entry])\n",
    "    else:\n",
    "        simplified_chambers.append(\"OTHER\")\n",
    "\n",
    "# Step 2: Update entries marked as \"OTHER\" using Hoja2\n",
    "for i, entry in enumerate(full_data[\"SOO\"]):\n",
    "    if simplified_chambers[i] == \"OTHER\" and isinstance(entry, str) and entry in map_2:\n",
    "        simplified_chambers[i] = map_2[entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_chamber(label):\n",
    "    \"\"\"\n",
    "    Normalize known chamber labels to: 'Left', 'Right', or 'OTHER'\n",
    "    \"\"\"\n",
    "    if label in [\"RVOT\", \"Right ventricle\", \"Tricuspid annulus\", \"Coronary sinus\"]:\n",
    "        return \"Right\"\n",
    "    elif label in [\"LVOT\", \"Left ventricle\", \"Mitral annulus\"]:\n",
    "        return \"Left\"\n",
    "    return \"OTHER\"\n",
    "\n",
    "final_chambers_normalized = [normalize_chamber(c) for c in simplified_chambers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the final label (Left, Right, OTHER) as a column in our metadata DataFrame\n",
    "df_meta[\"normalized_label\"] = final_chambers_normalized\n",
    "print(df_meta.info())\n",
    "\n",
    "df_clean = df_meta[df_meta[\"normalized_label\"].isin([\"Left\", \"Right\"])].copy()\n",
    "df_clean.info()\n",
    "print(df_clean[\"normalized_label\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many missing values in different columns, so we must decide how to handle them. Since no columns have more than 40% missing values, we decided to impute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute numerical columns (median)\n",
    "numeric_columns = [\"Age\", \"Height\", \"Weight\", \"BMI\"]\n",
    "df_clean[numeric_columns] = df_clean[numeric_columns].fillna(df_clean[numeric_columns].median())\n",
    "\n",
    "# Impute categorical columns (mode)\n",
    "categorical_columns = [\"Sex\", \"PVC_transition\", \"HTA\", \"DM\", \"DLP\", \"Smoker\", \"COPD\", \"Sleep_apnea\", \"OTorigin\"]\n",
    "df_clean[categorical_columns] = df_clean[categorical_columns].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "# Map labels to numeric values\n",
    "df_clean[\"label\"] = df_clean[\"normalized_label\"].map({\"Left\": 0, \"Right\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.info())\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract valid indices\n",
    "valid_indices = df_clean.index.tolist()\n",
    "labels = df_clean[\"label\"].values\n",
    "\n",
    "# Align ECG signals with df_clean\n",
    "X = [preprocessed_ecgs[i] for i in valid_indices]\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "    X, y, valid_indices, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert X_train and X_test to NumPy arrays\n",
    "X_train = np.stack(X_train)\n",
    "X_test = np.stack(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Check results\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class counts\n",
    "left_train = np.sum(y_train == 0)\n",
    "right_train = np.sum(y_train == 1)\n",
    "\n",
    "# Print counts\n",
    "print(\"Training Set:\")\n",
    "print(\"  Left (0):\", left_train)\n",
    "print(\"  Right (1):\", right_train)\n",
    "\n",
    "# Bar positions\n",
    "x = np.arange(2)  # Two bars: Left and Right\n",
    "width = 0.5\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.bar(x[0], left_train, width=width, label='Left (0)', color='red', edgecolor='black')\n",
    "plt.bar(x[1], right_train, width=width, label='Right (1)', color='blue', edgecolor='black')\n",
    "\n",
    "plt.xticks(x, ['Left (0)', 'Right (1)'])\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Load pretrained SAK models\n",
    "model_dir = \"data/modelos\"\n",
    "models = [torch.load(f\"{model_dir}/model.{i+1}\") for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import skimage.util\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "def predict_ecg(\n",
    "    ecg: np.ndarray,\n",
    "    fs: int = 250,\n",
    "    model: List[torch.nn.Module] = None,\n",
    "    window_size: int = 2048,\n",
    "    stride: int = 256,\n",
    "    threshold_ensemble: float = 0.5,\n",
    "    thr_dice: float = 0.9,\n",
    "    ptg_voting: float = 0.5,\n",
    "    batch_size: int = 16\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predicts P, QRS, and T masks for a given ECG signal using an ensemble of models.\n",
    "\n",
    "    Args:\n",
    "        ecg (np.ndarray): ECG signal of shape (T, L) where T is time, L is leads.\n",
    "        fs (int): Sampling frequency (default 250).\n",
    "        model (list): List of PyTorch models (each predicting sigmoid mask for 3 classes).\n",
    "        window_size (int): Window size in samples for model input.\n",
    "        stride (int): Stride between windows.\n",
    "        threshold_ensemble (float): Threshold proportion of models that must agree on a point.\n",
    "        thr_dice (float): Threshold for each model’s probability output to count as a “positive”.\n",
    "        ptg_voting (float): Final voting threshold on normalized votes per timepoint.\n",
    "        batch_size (int): Batch size for model prediction.\n",
    "\n",
    "    Returns:\n",
    "        full_mask (np.ndarray): Binary mask of shape (3, T) for P, QRS, and T waves.\n",
    "    \"\"\"\n",
    "    if ecg.shape[0] < 50:\n",
    "        raise ValueError(f\"Signal too short for segmentation: {ecg.shape}\")\n",
    "\n",
    "    ecg = np.copy(ecg)\n",
    "    if ecg.ndim == 2 and ecg.shape[0] < ecg.shape[1]:\n",
    "        ecg = ecg.T\n",
    "    ecg = ecg[:, :12]\n",
    "\n",
    "    N = ecg.shape[0]\n",
    "\n",
    "    # Pad to multiple of window size\n",
    "    if N < window_size:\n",
    "        pad = math.ceil(N / window_size) * window_size - N\n",
    "        ecg = np.pad(ecg, ((0, pad), (0, 0)), mode='edge')\n",
    "    \n",
    "    remainder = (ecg.shape[0] - window_size) % stride\n",
    "    if remainder != 0:\n",
    "        pad = stride - remainder\n",
    "        ecg = np.pad(ecg, ((0, pad), (0, 0)), mode='edge')\n",
    "\n",
    "    # Windowing\n",
    "    windowed = skimage.util.view_as_windows(ecg, (window_size, ecg.shape[1]), step=(stride, 1))\n",
    "    windowed = windowed[:, 0, :, :]  # (n_windows, 2048, 12)\n",
    "    windowed = np.swapaxes(windowed, 1, 2)  # (n_windows, 12, 2048)\n",
    "\n",
    "    # Model expects 1 channel per lead per window, so let's adjust the shape\n",
    "    windowed = windowed.reshape(-1, 1, window_size)  # (n_windows * 12, 1, 2048)\n",
    "\n",
    "    # Prediction\n",
    "    mask = np.zeros((windowed.shape[0], 3, window_size), dtype=float)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if model is None or len(model) == 0:\n",
    "        raise ValueError(\"No models provided for prediction.\")\n",
    "\n",
    "    # Set models to evaluation mode\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for m in model:\n",
    "            m = m.to(device)\n",
    "            for i in range(0, windowed.shape[0], batch_size):\n",
    "                inputs = {\"x\": torch.tensor(windowed[i:i+batch_size]).float().to(device)}\n",
    "                output = m(inputs)\n",
    "                \n",
    "                # Handle model outputs based on expected type\n",
    "                if isinstance(output, dict):\n",
    "                    outputs = output[\"sigmoid\"].cpu().numpy()\n",
    "                else:\n",
    "                    outputs = torch.sigmoid(output).cpu().numpy()\n",
    "                \n",
    "                mask[i:i+batch_size] += outputs > thr_dice\n",
    "\n",
    "        # Threshold based on ensemble agreement\n",
    "        mask = mask >= len(model) * threshold_ensemble\n",
    "\n",
    "    # Reconstruct full mask\n",
    "    full_len = (mask.shape[0] - 1) * stride + window_size\n",
    "    full_mask = np.zeros((3, full_len))\n",
    "    counter = np.zeros(full_len)\n",
    "\n",
    "    for i in range(mask.shape[0]):\n",
    "        start = i * stride\n",
    "        full_mask[:, start:start+window_size] += mask[i]\n",
    "        counter[start:start+window_size] += 1\n",
    "\n",
    "    # Avoid division by zero\n",
    "    counter[counter == 0] = 1\n",
    "    full_mask = (full_mask / counter) > ptg_voting\n",
    "\n",
    "    # Truncate to original length\n",
    "    full_mask = full_mask[:, :N]\n",
    "\n",
    "    return full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_morph_features(\n",
    "    signal: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    fs: int = 250\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Extracts R/S amplitude, ratio, QRS duration, and T polarity from a single lead.\n",
    "\n",
    "    Args:\n",
    "        signal (np.ndarray): ECG signal for one lead (T,) or (T, 1).\n",
    "        mask (np.ndarray): Binary segmentation mask of shape (3, T).\n",
    "        fs (int): Sampling frequency in Hz.\n",
    "\n",
    "    Returns:\n",
    "        dict: Extracted features.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    if signal.ndim == 2:\n",
    "        signal = signal[:, 0]\n",
    "    if signal.shape[0] < 10 or mask.shape != (3, signal.shape[0]):\n",
    "        raise ValueError(\"Malformed signal or mask\")\n",
    "\n",
    "    r_peak = np.max(signal)\n",
    "    s_trough = np.min(signal)\n",
    "    r_s_ratio = r_peak / abs(s_trough) if s_trough != 0 else 0\n",
    "\n",
    "    qrs_indices = np.where(mask[1])[0]\n",
    "    qrs_dur = (qrs_indices[-1] - qrs_indices[0]) / fs * 1000 if len(qrs_indices) > 1 else 0\n",
    "\n",
    "    t_indices = np.where(mask[2])[0]\n",
    "    if len(t_indices) > 3:\n",
    "        t_mean = np.mean(signal[t_indices])\n",
    "        polarity = 1 if t_mean > 0.02 else (-1 if t_mean < -0.02 else 0)\n",
    "    else:\n",
    "        polarity = 0\n",
    "\n",
    "    features[\"r_amp\"] = r_peak\n",
    "    features[\"s_amp\"] = s_trough\n",
    "    features[\"r_s_ratio\"] = r_s_ratio\n",
    "    features[\"qrs_dur\"] = qrs_dur\n",
    "    features[\"t_polarity\"] = polarity\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def extract_all_features(\n",
    "    X_data: np.ndarray,\n",
    "    y_data: np.ndarray,  # Patient ID or unique identifier\n",
    "    models: List[torch.nn.Module],\n",
    "    fs: int = 250\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract morphological features for all ECGs and all leads.\n",
    "\n",
    "    Args:\n",
    "        X_data (np.ndarray): ECGs of shape (N, T, L)\n",
    "        y_data (np.ndarray): Patient IDs or another identifier (aligned with X_data)\n",
    "        models (List[torch.nn.Module]): Ensemble of PyTorch models\n",
    "        fs (int): Sampling frequency\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Feature dataframe with one row per ECG, including patient_id\n",
    "    \"\"\"\n",
    "    feature_rows = []\n",
    "\n",
    "    for i in tqdm(range(len(X_data)), desc=\"Extracting features\"):\n",
    "        ecg = X_data[i]\n",
    "        patient_id = y_data[i]  # This corresponds to the patient ID or unique identifier\n",
    "\n",
    "        try:\n",
    "            # Predict mask or segmentation for the ECG (assuming you have a `predict_ecg` function)\n",
    "            mask = predict_ecg(ecg, fs=fs, model=models)\n",
    "\n",
    "            row = {\"patient_id\": patient_id}  # Start row with the patient ID\n",
    "\n",
    "            for lead_idx in range(ecg.shape[1]):\n",
    "                lead_signal = ecg[:, lead_idx]\n",
    "                lead_mask = mask.copy()\n",
    "\n",
    "                try:\n",
    "                    features = extract_morph_features(lead_signal, lead_mask, fs=fs)\n",
    "                    # Extract features for the current lead and add them to the row\n",
    "                    for key, val in features.items():\n",
    "                        row[f\"lead{lead_idx+1}_{key}\"] = val\n",
    "                except Exception as e:\n",
    "                    print(f\"Feature extraction failed on sample {i}, lead {lead_idx}: {e}\")\n",
    "                    # If feature extraction fails, fill with NaN\n",
    "                    for key in [\"r_amp\", \"s_amp\", \"r_s_ratio\", \"qrs_dur\", \"t_polarity\"]:\n",
    "                        row[f\"lead{lead_idx+1}_{key}\"] = np.nan\n",
    "\n",
    "            feature_rows.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Segmentation failed on sample {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Convert the list of rows into a DataFrame\n",
    "    feature_df = pd.DataFrame(feature_rows)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for the training set\n",
    "df_train_features = extract_all_features(X_train, train_idx, models)\n",
    "\n",
    "# Extract features for the test set\n",
    "df_test_features = extract_all_features(X_test, test_idx, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "X_train_feat = df_train_features.drop(columns=['patient_id'])\n",
    "y_train_labels = y_train\n",
    "\n",
    "# Apply ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=20)  # Select top 20 features\n",
    "X_train_selected = selector.fit_transform(X_train_feat, y_train_labels)\n",
    "\n",
    "# Retrieve selected feature names\n",
    "selected_features = X_train_feat.columns[selector.get_support()]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['patient_id'] = df_clean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_train_features with df_clean on 'patient_id'\n",
    "df_train_merged = pd.merge(df_train_features, df_clean, on='patient_id', how='left')\n",
    "\n",
    "# Exclude 'SOO' and 'normalized_label' from the categorical columns list for encoding\n",
    "categorical_cols = ['Sex', 'HTA', 'PVC_transition', 'SOO_chamber', 'DM', 'DLP', 'Smoker', 'COPD', 'Sleep_apnea']\n",
    "\n",
    "# Encode categorical columns using one-hot encoding\n",
    "df_train_merged = pd.get_dummies(df_train_merged, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Now df_train_merged contains both features and demographic data, ready for model training\n",
    "# Separate features and labels\n",
    "X_train_merged = df_train_merged.drop(columns=['patient_id', 'label', 'SOO', 'normalized_label'])\n",
    "y_train_merged = df_train_merged['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the test set\n",
    "df_test_merged = pd.merge(df_test_features, df_clean, on='patient_id', how='left')\n",
    "\n",
    "# Encode categorical columns using one-hot encoding for the test set\n",
    "df_test_merged = pd.get_dummies(df_test_merged, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Now df_test_merged contains both features and demographic data, ready for model training\n",
    "# Separate features for the test set\n",
    "X_test_merged = df_test_merged.drop(columns=['patient_id', 'label', 'SOO', 'normalized_label'])\n",
    "\n",
    "# Get the true labels for the test set directly from df_clean, based on patient_id\n",
    "y_test = df_clean.loc[df_clean['patient_id'].isin(df_test_merged['patient_id']), 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_merged, y_train_merged)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_merged)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False, xticklabels=['Left', 'Right'], yticklabels=['Left', 'Right'])\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.xlabel('Predicted', fontsize=14)\n",
    "plt.ylabel('True', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define the hyperparameter grid to search through (without 'max_features')\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],          # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],         # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],           # Minimum samples required at each leaf node\n",
    "}\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_merged, y_train_merged)\n",
    "\n",
    "# Get the best parameters from GridSearchCV\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_rf_model.predict(X_test_merged)\n",
    "\n",
    "# Evaluate performance of the best model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompBioMed25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
