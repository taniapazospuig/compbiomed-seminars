{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a9b28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.util\n",
    "import math\n",
    "import sak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a253f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SAK models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/5nns71_n7nx_v_2mkns5_k340000gn/T/ipykernel_50904/461035234.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  models = [torch.load(f\"{model_dir}/model.{i+1}\") for i in range(5)]\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained SAK models\n",
    "model_dir = \"data/modelos\"\n",
    "models = [torch.load(f\"{model_dir}/model.{i+1}\") for i in range(5)]\n",
    "print(\"Loaded SAK models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3bd8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df_signals with shape: (29153, 18)\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed signal array\n",
    "with open(\"processed_data_big_dataset/df_signals_preprocessed.pkl\", \"rb\") as f:\n",
    "    df_signals = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded df_signals with shape: {df_signals.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e143c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed_ecgs.pkl\n",
    "with open(\"processed_data_big_dataset/preprocessed_ecgs.pkl\", \"rb\") as f:\n",
    "    ecg_signals_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd1dc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ecg(ecg, fs=250, model=None, window_size=2048, stride=256, threshold_ensemble=0.5,\n",
    "                thr_dice=0.9, ptg_voting=0.5, batch_size=16):\n",
    "    \n",
    "    if ecg.shape[0] < 50: # Skip very short signals\n",
    "        raise ValueError(f\"Signal too short for segmentation: {ecg.shape}\")\n",
    "    \n",
    "    # Make sure shape is [time, leads]\n",
    "    ecg = np.copy(ecg)\n",
    "    if ecg.ndim == 2 and ecg.shape[0] < ecg.shape[1]:\n",
    "        ecg = ecg.T\n",
    "    ecg = ecg[:, :12]  # Only first 12 leads\n",
    "\n",
    "    # Pad if needed\n",
    "    N = ecg.shape[0]\n",
    "    # Pad to make the length a multiple of window size\n",
    "    if N < window_size:\n",
    "        pad = math.ceil(N / window_size) * window_size - N\n",
    "        ecg = np.pad(ecg, ((0, pad), (0, 0)), mode='edge')\n",
    "    # Also make sure the overlapping windows line up with the stride\n",
    "    if (ecg.shape[0] - window_size) % stride != 0:\n",
    "        pad = math.ceil((ecg.shape[0] - window_size) / stride) * stride - (ecg.shape[0] % window_size)\n",
    "        ecg = np.pad(ecg, ((0, pad), (0, 0)), mode='edge')\n",
    "\n",
    "    # Windowing\n",
    "    windowed = skimage.util.view_as_windows(ecg, (window_size, ecg.shape[1]), step=(stride, 1))\n",
    "    windowed = windowed[:, 0, :, :]  # Remove the singleton dimension (n_windows, 2048, 12)\n",
    "    windowed = np.swapaxes(windowed, 1, 2) # Shape becomes (n_windows, 12, 2048)\n",
    "\n",
    "    # Predict with models\n",
    "    # Each model outputs a prediction for 3 channels (P, QRS, T)\n",
    "    mask = np.zeros((windowed.shape[0], 3, window_size), dtype=int)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Loop over all models and batches\n",
    "    with torch.no_grad():\n",
    "        for m in model:\n",
    "            m = m.to(device)\n",
    "            for i in range(0, windowed.shape[0], batch_size):\n",
    "                inputs = {\"x\": torch.tensor(windowed[i:i+batch_size]).float().to(device)}\n",
    "                outputs = m(inputs)[\"sigmoid\"].cpu().numpy()\n",
    "                # Apply threshold to convert probabilities to binary\n",
    "                # Accumulate votes (how many models say \"yes\" to a region)\n",
    "                mask[i:i+batch_size] += outputs > thr_dice\n",
    "        # After all models vote, keep only regions where enough models agreed\n",
    "        mask = mask >= len(model) * threshold_ensemble\n",
    "\n",
    "    # Reconstruct full signal\n",
    "    full_mask = np.zeros((3, ecg.shape[0]))\n",
    "    counter = np.zeros(ecg.shape[0])\n",
    "    # For each window, place its mask into the right location in the full signal\n",
    "    for i in range(0, mask.shape[0]):\n",
    "        start = i * stride\n",
    "        full_mask[:, start:start+window_size] += mask[i]\n",
    "        counter[start:start+window_size] += 1 # Count how many times each timepoint has been covered since windows overlap\n",
    "    full_mask = (full_mask / counter) > ptg_voting # Normalize votes and apply final threshold for each timepoint\n",
    "    full_mask = full_mask[:, :N] # Trim to original signal length\n",
    "\n",
    "    # Clean up extra dimensions\n",
    "    if full_mask.ndim == 3 and full_mask.shape[-1] == 1:\n",
    "        full_mask = full_mask.squeeze(-1)\n",
    "\n",
    "    return full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0cffb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_morph_features(signal, mask, fs=250):\n",
    "    \"\"\"\n",
    "    Extract morphological features from a single-lead ECG and its segmentation mask.\n",
    "\n",
    "    Parameters:\n",
    "        signal (np.ndarray): ECG signal of shape (T, 1) or (T,)\n",
    "        mask (np.ndarray): Segmentation mask of shape (3, T)\n",
    "        fs (int): Sampling frequency in Hz\n",
    "\n",
    "    Returns:\n",
    "        dict: Morphological features extracted from the lead\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # Ensure 1D signal\n",
    "    if signal.ndim == 2 and signal.shape[1] == 1:\n",
    "        lead_signal = signal[:, 0]\n",
    "    elif signal.ndim == 1:\n",
    "        lead_signal = signal\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected signal shape: {signal.shape}\")\n",
    "\n",
    "    # Check mask shape\n",
    "    if mask.shape[0] != 3 or mask.shape[1] != lead_signal.shape[0]:\n",
    "        raise ValueError(f\"Signal or mask malformed: signal shape {lead_signal.shape}, mask shape {mask.shape}\")\n",
    "\n",
    "    # R and S amplitudes\n",
    "    r_peak = np.max(lead_signal)\n",
    "    s_trough = np.min(lead_signal)\n",
    "    r_s_ratio = r_peak / abs(s_trough) if s_trough != 0 else 0\n",
    "\n",
    "    # QRS duration (in milliseconds)\n",
    "    qrs_indices = np.where(mask[1])[0]\n",
    "    qrs_dur = (qrs_indices[-1] - qrs_indices[0]) / fs * 1000 if len(qrs_indices) > 1 else 0\n",
    "\n",
    "    # T wave polarity\n",
    "    t_indices = np.where(mask[2])[0]\n",
    "    if len(t_indices) > 3:\n",
    "        t_mean = np.mean(lead_signal[t_indices])\n",
    "        polarity = 1 if t_mean > 0.02 else (-1 if t_mean < -0.02 else 0)\n",
    "    else:\n",
    "        polarity = 0\n",
    "\n",
    "    features[\"r_amp\"] = r_peak\n",
    "    features[\"s_amp\"] = s_trough\n",
    "    features[\"r_s_ratio\"] = r_s_ratio\n",
    "    features[\"qrs_dur\"] = qrs_dur\n",
    "    features[\"t_polarity\"] = polarity\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abdbc3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Averaging ECGs per patient: 100%|██████████| 181/181 [00:00<00:00, 282.43it/s]\n",
      "Extracting features: 100%|██████████| 181/181 [05:27<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features_per_patient_avg.csv with shape: (181, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ECG Feature Extraction After Averaging Per Patient\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume we already have the ECGs loaded as a list\n",
    "# ecg_signals_all[i] corresponds to df_signals.iloc[i]['SampleID']\n",
    "\n",
    "# Group ECGs by PatientID\n",
    "patient_ecgs = {}\n",
    "for i, row in df_signals.iterrows():\n",
    "    pid = row.PatientID\n",
    "    if pid not in patient_ecgs:\n",
    "        patient_ecgs[pid] = []\n",
    "    patient_ecgs[pid].append(ecg_signals_all[i])\n",
    "\n",
    "# Average ECGs per patient (with shape validation)\n",
    "patient_avg_ecgs = {}\n",
    "for pid, ecgs in tqdm(patient_ecgs.items(), desc=\"Averaging ECGs per patient\"):\n",
    "    try:\n",
    "        shapes = [ecg.shape for ecg in ecgs]\n",
    "        if len(set(shapes)) > 1:\n",
    "            print(f\"Skipping patient {pid} due to mismatched ECG shapes: {set(shapes)}\")\n",
    "            continue\n",
    "        stacked = np.stack(ecgs, axis=0)\n",
    "        avg_ecg = np.mean(stacked, axis=0)\n",
    "        patient_avg_ecgs[pid] = avg_ecg\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping patient {pid} due to error: {e}\")\n",
    "\n",
    "# Visualize average vs original ECGs for one patient\n",
    "def plot_patient_avg_vs_originals(pid, lead_index=6):\n",
    "    if pid not in patient_ecgs or pid not in patient_avg_ecgs:\n",
    "        print(f\"Patient {pid} not found or not averaged.\")\n",
    "        return\n",
    "    originals = patient_ecgs[pid]\n",
    "    avg = patient_avg_ecgs[pid][:, lead_index]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, ecg in enumerate(originals):\n",
    "        plt.plot(ecg[:, lead_index], alpha=0.3, label=f\"ECG {i+1}\")\n",
    "    plt.plot(avg, color='black', linewidth=2, label=\"Averaged\")\n",
    "    plt.title(f\"Patient {pid} - Lead index {lead_index}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Feature extraction per patient average ECG \n",
    "features_list = []\n",
    "for pid, signal in tqdm(patient_avg_ecgs.items(), desc=\"Extracting features\"):\n",
    "    feats_all_leads = {}\n",
    "    try:\n",
    "        for j, lead_name in enumerate([\"I\", \"II\", \"III\", \"AVR\", \"AVL\", \"AVF\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\"]):\n",
    "            lead = signal[:, j]\n",
    "            if lead.ndim != 1 or lead.shape[0] < 50:\n",
    "                raise ValueError(f\"Invalid lead shape: {lead.shape}\")\n",
    "            lead = lead[:, np.newaxis]\n",
    "            mask = predict_ecg(lead, model=models)\n",
    "            if mask.ndim == 3:\n",
    "                mask = mask.squeeze(-1)\n",
    "            feats = extract_morph_features(lead, mask)\n",
    "            for k, v in feats.items():\n",
    "                feats_all_leads[f\"{lead_name}_{k}\"] = v\n",
    "        feats_all_leads[\"PatientID\"] = pid\n",
    "        features_list.append(feats_all_leads)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping patient {pid} due to error: {e}\")\n",
    "\n",
    "# Save results\n",
    "df_feats = pd.DataFrame(features_list)\n",
    "df_feats.to_csv(\"features_per_patient_avg.csv\", index=False)\n",
    "print(\"Saved features_per_patient_avg.csv with shape:\", df_feats.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompBioMed25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
